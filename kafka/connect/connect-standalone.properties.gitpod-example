# largely borrowed from datastax example config included in their kafka connect zip 

name=cassandra-sink
connector.class=com.datastax.oss.kafka.sink.CassandraSinkConnector
tasks.max=1
# NOTE might not be used 
plugin.path=/workspace/cassandra.realtime/kafka/connect/kafka-connect-cassandra-sink-1.4.0.jar

# Topics to subscribe to, comma-delimited.
topics=record-cassandra-leaves-avro

# some custom configs
# default is 500 ms, which is too low
# https://docs.datastax.com/en/kafka/doc/kafka/kafkaSettingJavaDriverConfig.html
datastax-java-driver.advanced.connection.init-query-timeout=2000 milliseconds

# The location of the cloud secure bundle used to connect to Datastax Astra.
# If you are connecting to the cloud, these parameters need to be provided.
# The loadBalancing.localDc, contactPoints or any config under ssl prefix cannot be
# set together with cloud.secureConnectBundle.
# In such a case a ConfigException will be thrown

# NOTE this is path within kafka connect docker container 
### TODO make sure to change! 
cloud.secureConnectBundle = /workspace/cassandra.realtime/kafka/connect/astra.credentials/secure-connect-<database-name-in-astra>.zip

# Comma-separated list of cluster contact points. Used by the DataStax Driver to discover
# cluster topology. Defaults to empty, meaning localhost.
# This setting cannot be provided if `cloud.secureConnectBundle` is provided, since this
# information is part of the bundle's data.
#contactPoints=

# Data center name to which the DataStax Driver connects. Required if contactPoints
# is specified.
# This setting cannot be provided if `cloud.secureConnectBundle` is provided, since this
# information is part of the bundle's data.
#loadBalancing.localDc=

# Native transport port to connect to on database nodes; defaults to 9042.
# If you need to configure different ports per host, do not use the contactPoints and port settings.
# Instead, you can leverage the ability to configure the underlying Java driver with the following setting:
# datastax-java-driver.basic.contact-points. This property takes a list of host:port pairs,
# so every contact point can have a different port, i.e:
#  datastax-java-driver.basic.contact-points = 127.0.0.1:9042, 127.0.0.2:9043
#port=9042

# Maximum number of requests to send to the database at a single time. Defaults to 500.
#maxConcurrentRequests=500

# Maximum number of records that could be send in one batch request to the database
#maxNumberOfRecordsInBatch=32

# Number of connections that driver maintains within a connection pool to each node in local dc
#connectionPoolLocalSize=4

# CQL statement execution timeout, in seconds. Defaults to 30 seconds.
#queryExecutionTimeout=30

# Whether or not to enable stats reporting through JMX. Defaults to true.
#jmx=true


# Specifies if the connector should ignore errors that occurred when processing the record.
# Possible values are:
# - None: never ignore errors
# - All: ignore all errors
# - Driver: ignore only driver errors (errors when writing to the database)
# Default is None.
#ignoreErrors=None

# Compression algorithm to use when issuing requests to the database. Valid values are
# None, Snappy, LZ4. Defaults to None.
#compression=None

### Authentication Setings ###

# Authentication provider to use, if any. Valid choices: None, PLAIN, GSSAPI.
# Defaults to None.
auth.provider=PLAIN

### TODO make sure to change! 
# Username for PLAIN authentication
auth.username=client-id

# Password for PLAIN authentication
auth.password=client-secret

# Kerberos keytab file (GSSAPI provider)
#auth.gssapi.keyTab=

# Kerberos principal (GSSAPI provider)
#auth.gssapi.principal=

# SASL service name to use for GSSAPI provider authentication.
# Defautls to dse.
#auth.gssapi.service=dse

### SSL Settings ###
# This entire section cannot be provided if `cloud.secureConnectBundle` is provided, since this
# information is part of the bundle's data.

# SSL provider to use, if any. Valid choices: None, JDK, OpenSSL.
# Defaults to None.
#ssl.provider=None

# Comma-separated list of cipher suites to enable.
#ssl.cipherSuites=

# Whether or not to validate node hostnames when using SSL.
# Defaults to true.
#ssl.hostnameValidation=true

# Keystore password.
#ssl.keystore.password=

# Path to the keystore file.
#ssl.keystore.path=

# Truststore password.
#ssl.truststore.password=

# Path to the truststore file.
#ssl.truststore.path=

# Path to the SSL certificate file, when using OpenSSL.
#ssl.openssl.keyCertChain=

# Path to the private key file, when using OpenSSL.
#ssl.openssl.privateKey=

### Settings for topic record-cassandra-leaves-avro ###
# see https://docs.datastax.com/en/kafka/doc/kafka/kafkaMapTopicTable.html
# For avro with Datastax kafka connector, see https://docs.datastax.com/en/kafka/doc/kafka/kafkaMapAvroMessages.html
#### Settings for table my_ks.leaves in topic record-cassandra-leaves-avro ####

# Kafka record field to database table column mapping spec, in the form 'col1=value.f1, col2=key.f1'.
# You can use special __ttl and __timestamp fields in the mapping to denote the fact that the field from kafka record should
# be inserted as the ttl or writetime of this database row.
# You can extract value from kafka record header using "header." suffix with the name of the header.
# You can use now() function in the mapping to generate TIMEUUID and insert it as a value to the given column.
#
# Example with (almost) all fields
# topic.record-cassandra-leaves-avro.<my_ks>.leaves.mapping = is_archived=value.is_archived, is_starred=value.is_starred, user_name=value.user_name, user_email=value.user_email, user_id=value.user_id, tags=value.tags, slugs=value.slugs, is_public=value.is_public, id=value.id, title=value.title, url=value.url, content_text=value.content_text, created_at=value.created_at, updated_at=value.updated_at, mimetype=value.mimetype, language=value.language, reading_time=value.reading_time, domain_name=value.domain_name, preview_picture=value.preview_picture, http_status=value.http_status, links=value.links, content=value.content 

# Working example for (at least most of) our 1000 records
# topic.record-cassandra-leaves-avro.<my_ks>.leaves.mapping = is_archived=value.is_archived, is_starred=value.is_starred, user_name=value.user_name, user_email=value.user_email, user_id=value.user_id, is_public=value.is_public, id=value.id, title=value.title, url=value.url, created_at=value.created_at, updated_at=value.updated_at, reading_time=value.reading_time, domain_name=value.domain_name

### TODO make sure to change! change all <my_ks> especially. This assumes db table is called "leaves". If that is not true, be sure to change that as well
# This includes the content field, which will cause some to not write correctly. Still works for nearly all records, which is good enough
topic.record-cassandra-leaves-avro.<my_ks>.leaves.mapping = tags=value.tags, is_archived=value.is_archived, is_starred=value.is_starred, user_name=value.user_name, user_email=value.user_email, user_id=value.user_id, is_public=value.is_public, id=value.id, title=value.title, url=value.url, created_at=value.created_at, updated_at=value.updated_at, reading_time=value.reading_time, domain_name=value.domain_name, content=value.content 



# You can provide own query to be executed when the new record to record-cassandra-leaves-avro will arrive - this is intended for advanced use cases.
# For the majority of use cases you should use standard mapping without `query` parameter.
# This query will be used instead of an auto-generated one and has priority over it.
# The bound variables used in the query needs to be provided in the mapping column, so for the given query:
# topic.record-cassandra-leaves-avro.<my_ks>.leaves.query=INSERT INTO %s.types (bigintCol, intCol) VALUES (:some_name, :some_name_2)
# mapping can look like this:
# topic.record-cassandra-leaves-avro.<my_ks>.leaves.mapping=some_name=value.bigint, some_name_2=value.int
#
# Please note that bound variables that can be used in the query may not be present in the database table.
# Due to that fact, when providing the query, validation of mapping columns is not performed.
# Also, the primary keys are not validated.
#
# You can leverage the query parameter to selectively update Maps or UDTs.
# To make it work, the target column must not be frozen.
# The update vs override behavior is defined via the nullToUnset parameter.

# TTL and TIMESTAMP support with `query` parameter:
# If you are using query parameter all TTL and TIMESTAMP related settings are ignored. There is no automatic extraction
# of write timestamp from Kafka record. If you wish to add those you need to take care of it
# using query parameter for example:
# `INSERT INTO %s.types (bigintCol, intCol) VALUES (:bigint_col, :int_col) USING TIMESTAMP :timestamp and TTL 1000`
#
# DELETES and `query` parameter:
# when using a user-provided query, DELETES operations are not supported - this is due to the fact that columns
# are not validated and the conditions for deletes cannot be inferred.
# If you want to provide a custom `query` parameter, you must set the `deletesEnabled` parameter to `false`.

# When you specify ttl value to be used while inserting (by mapping __ttl or .ttl static config)
# you may want to specify timeUnit of the provided ttl. Default is SECONDS.
# So if your field used as a ttl is in different format (for example HOURS) set it to HOURS.
# The connector will automatically transform it to proper database ttl format (to SECONDS)
# The transformation also applies for static ttl field config.
### TODO make sure to change! change all <my_ks> especially. 
topic.record-cassandra-leaves-avro.<my_ks>.leaves.ttlTimeUnit=SECONDS

# When you specify timestamp value to be used while inserting (by mapping __timestamp)
# you may want to specify timeUnit of the provided timestamp. Default is MICROSECONDS.
# So if your field used as a timestamp is in different format (for example HOURS) set it to HOURS.
# The connector will automatically transform it to proper database ttl format (to MICROSECONDS)
### TODO make sure to change! change all <my_ks> especially. 
topic.record-cassandra-leaves-avro.<my_ks>.leaves.timestampTimeUnit=MICROSECONDS

# Query consistency level; defaults to LOCAL_ONE.
#topic.record-cassandra-leaves-avro.<my_ks>.leaves.consistencyLevel=LOCAL_ONE

# Time-to-live. Set to the number of seconds before the data is automatically deleted in the
# database.
# Defaults to -1 (disabled).
#topic.record-cassandra-leaves-avro.<my_ks>.leaves.ttl=-1

# Whether to treat nulls in Kafka as UNSET in teh database. DataStax recommends using the default to
# avoid creating unnecessary tombstones. Defaults to true.
#topic.record-cassandra-leaves-avro.<my_ks>.leaves.nullToUnset=true

# When enabled, treat records that after mapping would result in only non-null values for
# primary key columns as deletes, rather than inserting/updating nulls for all regular columns.
# Note that for this behavior to trigger, the mapping specification must map all columns in the
# table. Defaults to true.
#topic.record-cassandra-leaves-avro.<my_ks>.leaves.deletesEnabled=true

#### Record decoding settings in topic record-cassandra-leaves-avro ####
# Locale to use for locale-sensitive conversions. Defaults to en_US.
#topic.record-cassandra-leaves-avro.codec.locale=en_US

# The time zone to use for temporal conversions that do not convey any explicit time zone
# information. Defaults to UTC.
#topic.record-cassandra-leaves-avro.codec.timeZone=UTC

# The temporal pattern to use for `String` to CQL `timestamp` conversion. Valid choices:
#
# - A date-time pattern such as `yyyy-MM-dd HH:mm:ss`.
# - A pre-defined formatter such as `ISO_ZONED_DATE_TIME` or `ISO_INSTANT`. Any public static
#   field in `java.time.format.DateTimeFormatter` can be used.
# - The special formatter `CQL_TIMESTAMP`, which is a special parser that accepts all valid CQL
#   literal formats for the `timestamp` type.
# Defaults to CQL_TIMESTAMP.
#topic.record-cassandra-leaves-avro.codec.timestamp=CQL_TIMESTAMP

# The temporal pattern to use for `String` to CQL `date` conversion. Valid choices:
#
# - A date-time pattern such as `yyyy-MM-dd`.
# - A pre-defined formatter such as `ISO_LOCAL_DATE`. Any public static field in
#   `java.time.format.DateTimeFormatter` can be used.
# Defaults to ISO_LOCAL_DATE.
#topic.record-cassandra-leaves-avro.codec.date=ISO_LOCAL_DATE

# The temporal pattern to use for `String` to CQL `time` conversion. Valid choices:
#
# - A date-time pattern, such as `HH:mm:ss`.
# - A pre-defined formatter, such as `ISO_LOCAL_TIME`. Any public static field in
#   `java.time.format.DateTimeFormatter` can be used.
# Defaults to ISO_LOCAL_TIME.
#topic.record-cassandra-leaves-avro.codec.time=ISO_LOCAL_TIME

# If the input is a string containing only digits that cannot be parsed using the
# `codec.timestamp` format, the specified time unit is applied to the parsed value. All
# `TimeUnit` enum constants are valid choices.
# Defaults to MILLISECONDS.
#topic.record-cassandra-leaves-avro.codec.unit=MILLISECONDS

# You can pass now all settings to the driver "datastax-java-driver" prefix directly.
# For example to pass basic.config-reload-interval, you need to add:
# datastax-java-driver.basic.config-reload-interval=1 minutes
# The value will be taken and propagated to the driver.
#
# The following connector settings are equivalent with datastax-java-driver prefix; if you set both, connector settings will take precedence over driver settings:
# - contactPoints             : datastax-java-driver.basic.contact-points
# - loadBalancing.localDc     : datastax-java-driver.basic.load-balancing-policy.local-datacenter
# - cloud.secureConnectBundle : datastax-java-driver.basic.cloud.secure-connect-bundle
# - queryExecutionTimeout     : datastax-java-driver.basic.request.timeout
# - connectionPoolLocalSize   : datastax-java-driver.advanced.connection.pool.local.size
# - compression               : datastax-java-driver.advanced.protocol.compression
# - metricsHighestLatency     : datastax-java-driver.advanced.metrics.session.cql-requests.highest-latency
# If you will not provide any of those, the default value for a connector setting will be taken.
#
# There is one difference between contactPoints and datastax-java-driver.basic.contact-points.
# For the the contactPoints the value of the port is appended to every host provided by this setting.
# For datastax-java-driver.basic.contact-points you need to provide full contact points (host:port).
# The second option gives more flexibility because you can specify different port for every host.
#
# All driver properties that are of a type `List`:
# - datastax-java-driver.advanced.ssl-engine-factory.cipher-suites
# - datastax-java-driver.advanced.metrics.node.enabled
# - datastax-java-driver.advanced.metadata.schema.refreshed-keyspaces
# - datastax-java-driver.advanced.metrics.session.enabled
# - datastax-java-driver.basic.contact-points
# will be automatically converted to the TypeSafe Config format used by the driver.
# The conversion will split values provided in those setting on the coma (,) sign and create indexed properties.
#
# For example if you will provide: datastax-java-driver.advanced.metrics.session.enabled=a,b
# this will be converted to:
# - datastax-java-driver.advanced.metrics.session.enabled.0=a
# - datastax-java-driver.advanced.metrics.session.enabled.1=b

